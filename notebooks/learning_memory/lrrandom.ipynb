{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import stable_baselines3 as sb3\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from dm_control.rl.control import PhysicsError\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from flygym.mujoco import Parameters\n",
    "from flygym.mujoco.rl import make_arena\n",
    "from flygym.mujoco.examples.turning_controller import HybridTurningNMF\n",
    "\n",
    "## Fix random seed =====\n",
    "np.random.seed(0)\n",
    "sb3.common.utils.set_random_seed(0, using_cuda=True)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(pt0, pt1):\n",
    "    rise = pt1[1] - pt0[1]\n",
    "    run = pt1[0] - pt0[0]\n",
    "    slope = rise / run\n",
    "    intercept = pt0[1] - pt0[0] * slope\n",
    "    return lambda x: slope * x + intercept\n",
    "\n",
    "class NMFNavigation(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arena_factory,\n",
    "        device=\"cpu\",\n",
    "        decision_dt=0.05,\n",
    "        max_time=5,\n",
    "        test_mode=False,\n",
    "        debug_mode=False,\n",
    "        spawn_x_range=(-2.5, 2.5),\n",
    "        spawn_orient_range=(np.pi / 2 - np.deg2rad(10), np.pi / 2 + np.deg2rad(10)),\n",
    "        descending_range=(0.2, 1),\n",
    "        tgt_margin_epsilon=2,\n",
    "        tgt_margin_q=3,\n",
    "        render_camera=\"birdeye_cam\",\n",
    "        render_playspeed=0.5,\n",
    "        vision_refresh_rate=None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        self.sim_params = Parameters(\n",
    "            timestep=1e-4,\n",
    "            render_mode=\"saved\",\n",
    "            render_playspeed=0.5,\n",
    "            render_window_size=(800, 608),\n",
    "            enable_olfaction=True,\n",
    "            enable_adhesion=True,\n",
    "            draw_adhesion=False,\n",
    "            render_camera=\"birdeye_cam\",\n",
    "        )  \n",
    "\n",
    "        self.device = device\n",
    "        self.spawn_y_range = spawn_x_range\n",
    "        self.spawn_orient_range = spawn_orient_range\n",
    "        self.arena_factory = arena_factory\n",
    "        self.tgt_margin_epsilon = tgt_margin_epsilon\n",
    "        self.tgt_margin_q = tgt_margin_q\n",
    "        self.controller_kwargs = kwargs\n",
    "        self.arena = self.arena_factory()\n",
    "        self.contact_sensor_placements =   [  f\"{leg}{segment}\"\n",
    "            for leg in [\"LF\", \"LM\", \"LH\", \"RF\", \"RM\", \"RH\"]\n",
    "            for segment in [\"Tibia\", \"Tarsus1\", \"Tarsus2\", \"Tarsus3\", \"Tarsus4\", \"Tarsus5\"]\n",
    "        ]\n",
    "\n",
    "        self.controller = HybridTurningNMF(\n",
    "            sim_params=self.sim_params,\n",
    "            arena=self.arena,\n",
    "            spawn_pos=(0, 0, 0.2),\n",
    "            contact_sensor_placements=self.contact_sensor_placements,\n",
    "            simulation_time=10,\n",
    "            detect_flip=False,\n",
    "            **self.controller_kwargs,\n",
    "        )\n",
    "\n",
    "        self.descending_range = descending_range\n",
    "         \n",
    "        self.odor_hist = []\n",
    "        self._x_pos_hist = []\n",
    "        #self._back_camera_x_offset = self.arena.back_cam.pos[0]\n",
    "\n",
    "        self.max_time = max_time\n",
    "        self.num_substeps = int(decision_dt / self.controller.timestep)\n",
    "         \n",
    "\n",
    "        # Override spaces\n",
    "        # action space: 2D vector of amplitude and phase for oscillators on each side\n",
    "        # observation space:\n",
    "        #  - 2D vector of x-y position of object relative to the fly, norm. to [0, 1]\n",
    "        #  - scalar probability that there is an object in view, [0, 1]\n",
    "        #  - 2D vector of mean odor intensity on each side, norm. to [0, 1]\n",
    "        #  - 2D vector of current oscillator amp. on each side, norm. to [0, 1]\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,))\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(10,))\n",
    "        \n",
    "        raw_obs = self.controller.get_observation()\n",
    "        fly_pos = raw_obs[\"fly\"][0, :2]\n",
    "        fly_heading = raw_obs[\"fly\"][2, 0] - np.pi / 2\n",
    "        tgt_pos = self.arena.odor_source[0, :2]\n",
    "        self._last_fly_tgt_dist = np.linalg.norm(fly_pos - tgt_pos)\n",
    "        self._last_score_tgt_heading = self._calc_heading_score(\n",
    "            fly_pos=fly_pos,\n",
    "            tgt_pos=tgt_pos,\n",
    "            fly_heading=fly_heading,\n",
    "        )\n",
    "\n",
    "    def turn_bias_to_descending_signal(self, turn_bias):\n",
    "        descending_span = self.descending_range[1] - self.descending_range[0]\n",
    "        descending_signal = np.ones((2,)) * self.descending_range[1]\n",
    "        if turn_bias < 0:\n",
    "            descending_signal[0] -= np.abs(turn_bias) * descending_span\n",
    "        else:\n",
    "            descending_signal[1] -= np.abs(turn_bias) * descending_span\n",
    "        return descending_signal\n",
    "    \n",
    "    def _calc_heading_score(\n",
    "            self,\n",
    "            fly_pos,\n",
    "            tgt_pos,\n",
    "            fly_heading,\n",
    "        ):\n",
    "        tgt_dir = np.arctan2(tgt_pos[1] - fly_pos[1], tgt_pos[0] - fly_pos[0])\n",
    "     \n",
    "        tgt_dir_rel = tgt_dir - fly_heading\n",
    "       \n",
    "        fly_tgt_dist = np.linalg.norm(fly_pos - tgt_pos)\n",
    "        tgt_ang_radius = np.arctan2(self.tgt_margin_epsilon, fly_tgt_dist)\n",
    "        \n",
    "        \n",
    "        func_tgt_heading = fit_line(\n",
    "            [tgt_ang_radius, 1], [self.tgt_margin_q * tgt_ang_radius, 0]\n",
    "        )\n",
    "        score_tgt_heading = func_tgt_heading(np.abs(tgt_dir_rel))\n",
    "        score_tgt_heading = np.clip(score_tgt_heading, 0, 1)\n",
    "\n",
    "        return score_tgt_heading\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, turn_bias):\n",
    "        ## Step physics =====\n",
    "        turning_signal = self.turn_bias_to_descending_signal(turn_bias)\n",
    "        try:\n",
    "            for i in range(self.num_substeps):\n",
    "                raw_obs, _, raw_term, raw_trunc, raw_info = self.controller.step(turning_signal)\n",
    "                \n",
    "                #back_cam = self.controller.arena.back_cam\n",
    "                # print(back_cam.pos)\n",
    "                # back_cam.pos[0] = raw_obs[\"fly\"][0, 0]\n",
    "                self._x_pos_hist.append(raw_obs[\"fly\"][0, 0])\n",
    "                #curr_cam_x_pos = back_cam.pos[0]\n",
    "                #if len(self._x_pos_hist) < 400:\n",
    "                #    smoothed_fly_pos = 0\n",
    "                #else:\n",
    "                #    smoothed_fly_pos = np.median(self._x_pos_hist[-800:])\n",
    "                #back_cam_x = max(curr_cam_x_pos, smoothed_fly_pos) + self._back_camera_x_offset\n",
    "                #self.controller.physics.bind(back_cam).pos[0] = back_cam_x\n",
    "                render_res = self.controller.render()\n",
    "                # if render_res is not None:\n",
    "                #     import matplotlib.pyplot as plt\n",
    "                #     plt.imshow(render_res)\n",
    "                #     plt.show()\n",
    "                #     assert False\n",
    "                if render_res is not None:\n",
    "                    self.odor_hist.append(raw_obs[\"odor_intensity\"].copy())\n",
    "        except PhysicsError:\n",
    "            print(\"Physics error, resetting environment\")\n",
    "            return np.zeros((10,), dtype=\"float32\"), 0, False, True, {}\n",
    "\n",
    "        ## Verify state of physics simulation =====\n",
    "        # check if visual inputs are rendered recently\n",
    "        time_since_update = self.controller.curr_time - self.controller._last_vision_update_time\n",
    "        assert time_since_update >= 0\n",
    "        assert time_since_update < 0.25 * self.controller.timestep or np.isinf(self.controller._last_vision_update_time)\n",
    "        # check if the fly state\n",
    "        has_flipped = raw_info[\"flip\"]\n",
    "        \n",
    "        ## Fetch variables for reward and obs calculation =====\n",
    "        fly_pos = raw_obs[\"fly\"][0, :2]\n",
    "        tgt_pos = self.arena.odor_source[0, :2]\n",
    "        fly_heading = raw_obs[\"fly\"][2, 0] - np.pi / 2 \n",
    "        tgt_dir = np.arctan2(tgt_pos[1] - fly_pos[1], tgt_pos[0] - fly_pos[0])\n",
    "        \n",
    "        tgt_dir_rel = tgt_dir - fly_heading\n",
    "         \n",
    "        fly_tgt_dist = np.linalg.norm(fly_pos - tgt_pos) \n",
    "        tgt_ang_radius = np.arctan2(self.tgt_margin_epsilon, fly_tgt_dist)\n",
    "        \n",
    "        ## Calculate tentative costs\n",
    "        func_tgt_heading = fit_line([tgt_ang_radius, 1], [self.tgt_margin_q * tgt_ang_radius, 0])\n",
    "        score_tgt_heading = func_tgt_heading(np.abs(tgt_dir_rel))\n",
    "        score_tgt_heading = np.clip(score_tgt_heading, 0, 1)\n",
    "        score_tgt_heading_2 = self._calc_heading_score(\n",
    "            fly_pos, tgt_pos, fly_heading\n",
    "        )  # some refactorign needed\n",
    "        assert score_tgt_heading == score_tgt_heading_2\n",
    "        \n",
    "        ## Calculate reward and termination/truncation state =====\n",
    "        k_dist = 1\n",
    "        k_avoid = 7\n",
    "        k_attract = 1 \n",
    "        r_success = 10\n",
    "        r_fail = -5\n",
    "        r_dist = k_dist * (self._last_fly_tgt_dist - fly_tgt_dist)\n",
    "         \n",
    "        r_attract = k_attract * (score_tgt_heading - self._last_score_tgt_heading)\n",
    "\n",
    "        # decide final reward and terminating states by case\n",
    "        info = {}\n",
    "        if fly_tgt_dist < self.tgt_margin_epsilon:\n",
    "            reward = r_success\n",
    "            terminated = True\n",
    "            info[\"state_desc\"] = \"success\"\n",
    "        elif has_flipped:\n",
    "            reward = r_fail\n",
    "            terminated = True\n",
    "            info[\"state_desc\"] = \"flipped\"\n",
    "        else:\n",
    "            reward = r_dist + r_attract \n",
    "            terminated = False\n",
    "            info[\"state_desc\"] = \"seeking\"\n",
    "        \n",
    "        # decide timeout condition\n",
    "        if self.controller.curr_time > self.max_time and not terminated:\n",
    "            truncated = True\n",
    "            info[\"state_desc\"] = \"timeout\"\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        # Make observation =====\n",
    "         \n",
    "        turn_bias_norm = turn_bias[0] / 2 + 0.5\n",
    "         \n",
    "        odor_intensity = np.average(\n",
    "            raw_obs[\"odor_intensity\"][0, :].reshape(2, 2), axis=0, weights=[9, 1]\n",
    "        )\n",
    "        odor_intensity /= self.arena.peak_odor_intensity[0, 0]\n",
    "        odor_intensity = np.clip(np.sqrt(odor_intensity), 0, 1)\n",
    "        obs = np.array(\n",
    "            [0, 0, 0, 0, 0, 0, 0, *odor_intensity, turn_bias_norm], dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        ## Update state ===== \n",
    "        self._last_score_tgt_heading = score_tgt_heading\n",
    "        self._last_fly_tgt_dist = fly_tgt_dist\n",
    "\n",
    "        ## Prepare debugging info =====\n",
    "        info[\"fly_pos\"] = fly_pos \n",
    "        info[\"tgt_pos\"] = tgt_pos\n",
    "        info[\"fly_heading\"] = fly_heading \n",
    "        info[\"tgt_dir\"] = tgt_dir\n",
    "        info[\"tgt_dir_rel\"] = tgt_dir_rel \n",
    "        info[\"fly_tgt_dist\"] = fly_tgt_dist \n",
    "        info[\"score_tgt_heading\"] = score_tgt_heading\n",
    "        info[\"r_dist\"] = r_dist \n",
    "        info[\"r_attract\"] = r_attract\n",
    "        info[\"r_total\"] = reward\n",
    "       \n",
    "        info[\"odor_intensity\"] = odor_intensity\n",
    "        info[\"turn_bias\"] = turn_bias\n",
    "        info[\"terminated\"] = terminated\n",
    "        info[\"truncated\"] = truncated\n",
    "        if self.debug_mode:\n",
    "            print(\"=======================\")\n",
    "            for k, v in info.items():\n",
    "                print(f\"  * {k}: {v}\")\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=0, spawn_pos=None, spawn_orient=None):\n",
    "        if self.spawn_y_range is not None and spawn_pos is None:\n",
    "            spawn_pos = np.array([0, np.random.uniform(-5, 5), 0.2])\n",
    "        if self.spawn_orient_range is not None and spawn_orient is None:\n",
    "            spawn_yaw = np.random.uniform(\n",
    "                self.spawn_orient_range[0], self.spawn_orient_range[1]\n",
    "            )\n",
    "            spawn_orient = np.array([0, 0, spawn_yaw])\n",
    "        kwargs = copy.deepcopy(self.controller_kwargs)\n",
    "        if spawn_pos is not None:\n",
    "            kwargs[\"spawn_pos\"] = spawn_pos\n",
    "        if spawn_orient is not None:\n",
    "            kwargs[\"spawn_orient\"] = spawn_orient\n",
    "        self.controller.close()\n",
    "        self.arena = self.arena_factory()\n",
    "        self.controller = HybridTurningNMF(\n",
    "            sim_params=self.sim_params,\n",
    "            arena=self.arena,\n",
    "            spawn_pos=(0, 0, 0.2),\n",
    "            contact_sensor_placements=self.contact_sensor_placements,\n",
    "            simulation_time=10,\n",
    "            detect_flip=True,\n",
    "            **self.controller_kwargs,\n",
    "        )\n",
    "        obs = np.zeros((10,), dtype=\"float32\")\n",
    "\n",
    "        raw_obs = self.controller.get_observation()\n",
    "        fly_pos = raw_obs[\"fly\"][0, :2]\n",
    "        fly_heading = raw_obs[\"fly\"][2, 0] - np.pi / 2\n",
    "        tgt_pos = self.arena.odor_source[0, :2]\n",
    "        self._last_fly_tgt_dist = np.linalg.norm(fly_pos - tgt_pos)\n",
    "        self._last_score_tgt_heading = self._calc_heading_score(\n",
    "            fly_pos=fly_pos,\n",
    "            tgt_pos=tgt_pos,\n",
    "            fly_heading=fly_heading,\n",
    "        )\n",
    "        if self.debug_mode:\n",
    "            print(\"resetting environment\")\n",
    "        return obs, {\"state_desc\": \"reset\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = NMFNavigation(\n",
    "            arena_factory=make_arena,\n",
    "            test_mode=False,\n",
    "            debug_mode=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"data/rl/rl_model.zip\"\n",
    "model = sb3.SAC.load(model_path)\n",
    "reward_hist = []\n",
    "action_hist = []\n",
    "obs, info = sim.reset()\n",
    "obs_hist = [obs]\n",
    "info_hist = [info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(100):\n",
    "    action, _ = model.predict(obs) \n",
    "\n",
    "    obs, reward, terminated, truncated, info = sim.step(action)\n",
    "    action_hist.append(action)\n",
    "    obs_hist.append(obs)\n",
    "    reward_hist.append(reward)\n",
    "    info_hist.append(info)\n",
    "    if info[\"fly_tgt_dist\"] < 2.5:\n",
    "        print(\"distance < 3, stopping\")\n",
    "        break\n",
    "    if terminated:\n",
    "        print(\"terminated\")\n",
    "        break\n",
    "obs_hist = np.array(obs_hist)\n",
    "reward_hist = np.array(reward_hist)\n",
    "action_hist = np.array(action_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(100):\n",
    "    action, _ = model.predict(obs) \n",
    "    print(action)\n",
    "    sim.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"rl_first_attempt\"\n",
    "out_path = Path(f\"./outputs/plots/{name}\")\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = sim.controller.get_observation()\n",
    "ob[\"odor_intensity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odor_intensity = np.average(\n",
    "            ob[\"odor_intensity\"][0, :].reshape(2, 2), axis=0, weights=[9, 1]\n",
    "        )\n",
    "odor_intensity /= sim.arena.peak_odor_intensity[0, 0]\n",
    "odor_intensity = np.clip(np.sqrt(odor_intensity), 0, 1)\n",
    "print(odor_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sim.controller._frames)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4), tight_layout=True)\n",
    "ax.imshow(sim.controller._frames[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTEMPT TO TRAIN A MODEL**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_procs = 19\n",
    "def make_env():\n",
    "        sim = NMFNavigation(\n",
    "            arena_factory=make_arena,\n",
    "            test_mode=False,\n",
    "            debug_mode=False,\n",
    "        )\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import stable_baselines3 as sb3\n",
    "import stable_baselines3.common.logger as logger\n",
    "import stable_baselines3.common.callbacks as callbacks\n",
    "from pathlib import Path\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "\n",
    "\n",
    "print(\"Making vector env\")\n",
    "vec_env = make_vec_env(make_env, n_envs=num_procs, vec_env_cls=SubprocVecEnv)\n",
    "print(\"Vector env created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sb3.common.utils.set_random_seed(0, using_cuda=True)\n",
    "base_dir = Path(\"/home/nmf-learning/flygym-scratch/flygym/data\")\n",
    "start_from = None\n",
    "\n",
    "log_dir = str(base_dir / \"logs/\")\n",
    "checkpoint_callback = callbacks.CheckpointCallback(\n",
    "    save_freq=100,\n",
    "    save_path=log_dir,\n",
    "    name_prefix=base_dir.name,\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True,\n",
    "    verbose=2,\n",
    ")\n",
    "my_logger = logger.configure(log_dir, [\"tensorboard\", \"stdout\", \"csv\"])\n",
    "if start_from is None:\n",
    "    model = sb3.SAC(\n",
    "        \"MlpPolicy\",\n",
    "        # env=sim,\n",
    "        env=vec_env,\n",
    "        policy_kwargs={\"net_arch\": [32, 32]},\n",
    "        verbose=2,\n",
    "        learning_rate=0.01,\n",
    "    )\n",
    "else:\n",
    "    model = sb3.SAC.load(start_from)\n",
    "    model.set_env(vec_env)\n",
    "    print(model.verbose, model.learning_rate, model.policy_kwargs)\n",
    "model.set_logger(my_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training start\")\n",
    "import copy\n",
    "model.learn(total_timesteps=500_000, progress_bar=False, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
